{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 3: Vision Transformers\n",
    "\n",
    "**Open notebook:** \n",
    "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/phlippe/asci_cbl_practicals/blob/main/notebooks/3_Vision_Transformers.ipynb)\n",
    "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/phlippe/asci_cbl_practicals/blob/main/notebooks/3_Vision_Transformers.ipynb)    \n",
    "**Authors:** Phillip Lippe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical, we will take a closer look at a recent new trend: Transformers for Computer Vision. Since [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) successfully applied a Transformer on a variety of image recognition benchmarks, there have been an incredible amount of follow-up works showing that CNNs might not be optimal architecture for Computer Vision anymore. But how do Vision Transformers work exactly, and what benefits and drawbacks do they offer in contrast to CNNs? We will answer these questions by implementing a Vision Transformer ourselves and train it on the popular, small dataset CIFAR10.\n",
    "\n",
    "Let's start with importing our standard set of libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "from PIL import Image\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "# PyTorch Lightning\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.6\n",
    "    import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# Import tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/practical3\"\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the CIFAR10 dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Dataset statistics for normalizing the input values to zero mean and one std\n",
    "DATA_MEANS = [0.491, 0.482, 0.447]\n",
    "DATA_STD = [0.247, 0.243, 0.261]\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "# Create data loaders for later. Adjust batch size if you have a smaller GPU\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=3)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=3)\n",
    "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For later, we keep a set of example images\n",
    "CIFAR_images = torch.stack([val_set[idx][0] for idx in range(4)], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=4, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Image examples of the CIFAR10 dataset\", fontsize=20)\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Building a Transformer for image classification\n",
    "\n",
    "Transformers have been originally proposed to process sets since it is a permutation-equivariant architecture, i.e., producing the same output permuted if the input is permuted. To apply Transformers to sequences, one commonly adds a positional encoding to the input feature vectors, and the model learns by itself what to do with it. So, why not do the same thing on images? This is exactly what [Alexey Dosovitskiy et al.](https://openreview.net/pdf?id=YicbFdNTTy) proposed in their paper \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\". Specifically, the Vision Transformer is a model for image classification that views images as sequences of smaller patches. As a preprocessing step, we split an image of, for example, $48\\times 48$ pixels into 9 $16\\times 16$ patches. Each of those patches is considered to be a \"word\"/\"token\" and projected to a feature space. With adding positional encodings and a token for classification on top, we can apply a Transformer as usual to this sequence and start training it for our task. A nice GIF visualization of the architecture is shown below (figure credit - [Phil Wang](https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif)):\n",
    "\n",
    "<center width=\"100%\"><img src=\"figures/vit.gif\" width=\"600px\"></center>\n",
    "\n",
    "We will walk step by step through the Vision Transformer, and implement all parts by ourselves. First, let's implement the image preprocessing: an image of size $N\\times N$ has to be split into $(N/M)^2$ patches of size $M\\times M$. These represent the input words to the Transformer. Implement this logic in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    Outputs:\n",
    "        Tensor of shape [B, H*W/patch_size^2, C, patch_size, patch_size] if flatten_channels=False,\n",
    "        and [B, H*W/patch_size^2, C*patch_size^2] otherwise.\n",
    "    \"\"\"\n",
    "    # TODO: Implement the splitting of images into patches\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.randn(4, 3, 48, 48)\n",
    "patch_size = 16\n",
    "out = img_to_patch(imgs, patch_size, flatten_channels=False)\n",
    "assert len(out.shape) == 5\n",
    "assert out.shape[0] == imgs.shape[0]\n",
    "assert out.shape[1] == imgs.shape[2]*imgs.shape[3]/patch_size**2\n",
    "assert out.shape[2] == imgs.shape[1]\n",
    "assert out.shape[3] == patch_size\n",
    "assert out.shape[4] == patch_size\n",
    "\n",
    "out = img_to_patch(imgs, patch_size, flatten_channels=True)\n",
    "assert len(out.shape) == 3\n",
    "assert out.shape[0] == imgs.shape[0]\n",
    "assert out.shape[1] == imgs.shape[2]*imgs.shape[3]/patch_size**2\n",
    "assert out.shape[2] == imgs.shape[1]*patch_size**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how that works for our CIFAR examples above. For our images of size $32\\times 32$, we choose a patch size of 4. Hence, we obtain sequences of 64 patches of size $4\\times 4$. We visualize them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n",
    "\n",
    "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14,3))\n",
    "fig.suptitle(\"Images as input sequences of patches\")\n",
    "for i in range(CIFAR_images.shape[0]):\n",
    "    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=64, normalize=True, pad_value=0.9)\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "    ax[i].imshow(img_grid)\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the original images, it is much harder to recognize the objects from those patch lists now. Still, this is the input we provide to the Transformer for classifying the images. The model has to learn itself how it has to combine the patches to recognize the objects. The inductive bias in CNNs that an image is a grid of pixels, is lost in this input format.\n",
    "\n",
    "After we have looked at the preprocessing, we can now start building the Transformer model. You can make use of the PyTorch module `nn.MultiheadAttention` ([docs](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html?highlight=multihead#torch.nn.MultiheadAttention)) here. Further, we use the Pre-Layer Normalization version of the Transformer blocks proposed by [Ruibin Xiong et al.](http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf) in 2020. The idea is to apply Layer Normalization not in between residual blocks, but instead as a first layer in the residual blocks. This reorganization of the layers supports better gradient flow and removes the necessity of a warm-up stage. A visualization of the difference between the standard Post-LN and the Pre-LN version is shown below.\n",
    "\n",
    "<center width=\"100%\"><img src=\"figures/pre_layer_norm.svg\" width=\"400px\"></center>\n",
    "\n",
    "First, implement a Pre-LN attention block below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network \n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Implement an pre-LN attention block\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input tensor of shape [Num Patches, Batch size, embed_dim]\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the attention block\n",
    "num_heads = np.random.randint(low=4, high=16)\n",
    "embed_dim = num_heads * np.random.randint(low=16, high=32)\n",
    "hidden_dim = np.random.randint(low=128, high=512)\n",
    "block = AttentionBlock(embed_dim, hidden_dim, num_heads)\n",
    "block.to(device)\n",
    "inp = torch.randn(10, 32, embed_dim, device=device)\n",
    "out = block(inp)\n",
    "for i in range(len(inp.shape)):\n",
    "    assert out.shape[i] == inp.shape[i]\n",
    "\n",
    "# Checking whether batch and patch dimension are correct\n",
    "inp2 = inp.clone()\n",
    "inp2[:,0] = 0.0\n",
    "out2 = block(inp2)\n",
    "diff = (out - out2).abs()\n",
    "assert (diff[:,0] > 1e-4).any(), 'Output tensor shows no difference although input has changed'\n",
    "assert (diff[:,1:] < 1e-4).all(), 'Other tensors besides the changed batch element have altered outputs. Check the dimensions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all modules ready to build our own Vision Transformer. Besides the Transformer encoder, we need the following modules:\n",
    "\n",
    "* A **linear projection** layer that maps the input patches to a feature vector of larger size. It is implemented by a simple linear layer that takes each $M\\times M$ patch independently as input.\n",
    "* A **classification token** that is added to the input sequence. We will use the output feature vector of the classification token (CLS token in short) for determining the classification prediction.\n",
    "* Learnable **positional encodings** that are added to the tokens before being processed by the Transformer. Those are needed to learn position-dependent information, and convert the set to a sequence. Since we usually work with a fixed resolution, we can learn the positional encodings instead of having the pattern of sine and cosine functions.\n",
    "* An **MLP head** that takes the output feature vector of the CLS token, and maps it to a classification prediction. This is usually implemented by a small feed-forward network or even a single linear layer.\n",
    "\n",
    "With those components in mind, let's implement the full Vision Transformer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and \n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: Implement all elements of the full Vision Transform\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement the forward pass\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Vision Transformer module\n",
    "num_heads = np.random.randint(low=4, high=16)\n",
    "embed_dim = num_heads * np.random.randint(low=16, high=32)\n",
    "hidden_dim = np.random.randint(low=128, high=512)\n",
    "num_channels = 3\n",
    "num_layers = np.random.randint(low=2, high=4)\n",
    "num_classes = np.random.randint(low=5, high=20)\n",
    "patch_size = [2,4,8][np.random.randint(low=0, high=3)]\n",
    "num_patches = int((32/patch_size)**2)\n",
    "\n",
    "vit_module = VisionTransformer(embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches)\n",
    "vit_module.to(device)\n",
    "imgs = torch.randn(4, 3, 32, 32, device=device)\n",
    "out = vit_module(imgs)\n",
    "assert out.shape[0] == imgs.shape[0]\n",
    "assert out.shape[1] == num_classes\n",
    "\n",
    "# Checking whether batch and patch dimension are correct\n",
    "imgs2 = imgs.clone()\n",
    "imgs2[0] = 0.0\n",
    "out2 = vit_module(imgs2)\n",
    "diff = (out2 - out).abs()\n",
    "assert (diff[0] > 1e-4).any(), 'Output tensor shows no difference although input has changed'\n",
    "assert (diff[1:] < 1e-4).all(), 'Other tensors besides the changed batch element have altered outputs. Check the dimensions'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put everything into a PyTorch Lightning Module as usual. We use `torch.optim.AdamW` as the optimizer, which is Adam with a corrected weight decay implementation. Since we use the Pre-LN Transformer version, we do not need to use a learning rate warmup stage anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = VisionTransformer(**model_kwargs)\n",
    "        self.example_input_array = next(iter(train_loader))[0]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=1e-3)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75, 90], gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]   \n",
    "    \n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        # TODO: Implement step to calculate the loss and accuracy for a batch\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Commonly, Vision Transformers are applied to large-scale image classification benchmarks such as ImageNet to leverage their full potential. However, here we take a step back and ask: can Vision Transformer also succeed on classical, small benchmarks such as CIFAR10? To find this out, we train a Vision Transformer from scratch on the CIFAR10 dataset. Let's first create a training function for our PyTorch Lightning module which also loads the pre-trained model if you have downloaded it above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_epochs=100, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"ViT\"), \n",
    "                         gpus=1 if str(device)==\"cuda:0\" else 0,\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\"),\n",
    "                                    LearningRateMonitor(\"epoch\")],\n",
    "                         check_val_every_n_epoch=10)\n",
    "    trainer.logger._log_graph = True         # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ViT.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f\"Found pretrained model at {pretrained_filename}, loading...\")\n",
    "        model = ViT.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = ViT(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can already start training our model. As seen in our implementation, we have a couple of hyperparameters that we have to set. When creating this notebook, we have performed a small grid search over hyperparameters and listed the best hyperparameters in the cell below. Nevertheless, it is worth discussing the influence that each hyperparameter has, and what intuition we have for choosing its value.\n",
    "\n",
    "First, let's consider the patch size. The smaller we make the patches, the longer the input sequences to the Transformer become. While in general, this allows the Transformer to model more complex functions, it requires a longer computation time due to its quadratic memory usage in the attention layer. Furthermore, small patches can make the task more difficult since the Transformer has to learn which patches are close-by, and which are far away. We experimented with patch sizes of 2, 4, and 8 which gives us the input sequence lengths of 256, 64, and 16 respectively. We found 4 to result in the best performance and hence pick it below. \n",
    "\n",
    "Next, the embedding and hidden dimensionality have a similar impact on a Transformer as to an MLP. The larger the sizes, the more complex the model becomes, and the longer it takes to train. In Transformers, however, we have one more aspect to consider: the query-key sizes in the Multi-Head Attention layers. Each key has the feature dimensionality of `embed_dim/num_heads`. Considering that we have an input sequence length of 64, a minimum reasonable size for the key vectors is 16 or 32. Lower dimensionalities can restrain the possible attention maps too much. To reduce the computational complexity, we recommend using 4 heads, 128 embedding dimensionality and 256 hidden dimensionality for a start.\n",
    "\n",
    "Finally, the learning rate for Transformers is usually relatively small, and in papers, a common value to use is 3e-5. However, since we work with a smaller dataset and have a potentially easier task, we found that we are able to increase the learning rate to 3e-4 without any problems.\n",
    "\n",
    "Feel free to explore the hyperparameters yourself by changing the values below. For a final run for the report, increase the epochs to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, results = train_model(model_kwargs={\n",
    "                                'embed_dim': 128,\n",
    "                                'hidden_dim': 256,\n",
    "                                'num_heads': 8,\n",
    "                                'num_layers': 6,\n",
    "                                'patch_size': 8,\n",
    "                                'num_channels': 3,\n",
    "                                'num_patches': 16,\n",
    "                                'num_classes': 10,\n",
    "                                'dropout': 0.0\n",
    "                              },\n",
    "                              lr=3e-4,\n",
    "                              max_epochs=10)\n",
    "print(\"ViT results\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n",
    "%tensorboard --logdir ../saved_models/practical3/ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What accuracy does the ViT achieve? How does this compare to the CNN you have implemented in the second assignment? Add the plots of the validation accuracy and a discussion of the previous questions in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 1: Importance of Positional Embeddings\n",
    "\n",
    "This part of the practical is not mandatory, but we recommend going through it if you have time. Here, we are interested in what elements of a Transformer are crucial. One considerable difference to CNNs is that Transformers look at the images as patches instead of a full grid, which removes some inductive biases about the positional information. To still have access to positional information, we use position embeddings. However, how important are those to the Transformer actually? Do we see a noticable accuracy drop if we don't use the position embeddings? Or is it looking at the images as a big bag of words anyways? Finally, how does this relation change when using different patch sizes? These questions you should try to find an answer in this part of the practical. Specifically, train a Transformer without positional embeddings, and compare it to your original Transformer. Repeat the experiment for a smaller patch size (4 or even 2) and compare how the accuracies have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we have implemented our own Vision Transformer from scratch and applied it to the task of image classification. Vision Transformers work by splitting an image into a sequence of smaller patches, use those as input to a standard Transformer encoder. While Vision Transformers achieved outstanding results on large-scale image recognition benchmarks such as ImageNet, they considerably underperform when being trained from scratch on small-scale datasets like CIFAR10. The reason is that in contrast to CNNs, Transformers do not have the inductive biases of translation invariance and the feature hierarchy (i.e. larger patterns consist of many smaller patterns). However, these aspects can be learned when enough data is provided, or the model has been pre-trained on other large-scale tasks. Considering that Vision Transformers have just been proposed end of 2020, there is likely a lot more to come on Transformers for Computer Vision.\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" International Conference on Representation Learning (2021). [link](https://arxiv.org/pdf/2010.11929.pdf)\n",
    "\n",
    "Chen, Xiangning, et al. \"When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations.\" arXiv preprint arXiv:2106.01548 (2021). [link](https://arxiv.org/abs/2106.01548)\n",
    "\n",
    "Tolstikhin, Ilya, et al. \"MLP-mixer: An all-MLP Architecture for Vision.\" arXiv preprint arXiv:2105.01601 (2021). [link](https://arxiv.org/abs/2105.01601)\n",
    "\n",
    "Xiong, Ruibin, et al. \"On layer normalization in the transformer architecture.\" International Conference on Machine Learning. PMLR, 2020. [link](http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
